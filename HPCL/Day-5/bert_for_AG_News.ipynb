{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75qAyvC1st2z"
      },
      "source": [
        "## <font color='blue'>**BERT (Bidirectional Encoder Representations from Transformers)**</font>\n",
        "\n",
        "https://arxiv.org/pdf/1810.04805.pdf\n",
        "\n",
        "### <font color='red'>**Overview:** </font>\n",
        "BERT is a pre-trained model that learns contextual word representations by considering both left and right context in a sentence. Unlike RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks), which process sequential data sequentially, BERT utilizes the Transformer architecture, enabling efficient computation through parallel processing.\n",
        "\n",
        "### <font color='red'>**Pretraining:** </font>\n",
        "During pretraining, BERT learns contextual representations through masked language modeling and next sentence prediction tasks, leveraging large-scale unlabeled data.\n",
        "### <font color='red'>**Fine-tuning:** </font>\n",
        "After pretraining, BERT can be fine-tuned on downstream tasks by adding task-specific layers and training on labeled data. Fine-tuning BERT's pretrained representations often leads to state-of-the-art performance on various natural language processing tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltFFibAQCSd5"
      },
      "source": [
        "![Pre-training and Fine-tuning](https://github.com/Rekha215/Pytorch-Basics/blob/main/Screenshot%202024-04-04%20130426.png?raw=true)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O12sP_wtC2xf"
      },
      "source": [
        "**Token embeddings** encode the meaning of individual words, **segment embeddings** distinguish between different segments of text, and **position embeddings** encode the sequential order of tokens within a sequence.\n",
        "\n",
        "![Embeddings](https://github.com/Rekha215/Pytorch-Basics/blob/main/Screenshot%202024-04-04%20130513.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unBYyY_3H7xT",
        "outputId": "ddafa93b-b748-4464-b107-c3e583ec0d61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text\n",
        "text = \"this is the last session of AI/ML course.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens)\n",
        "\n",
        "# Tokenize input\n",
        "input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "print(input_ids)\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "input_ids = torch.tensor([input_ids])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmspImH9H_Oe",
        "outputId": "f01d6a41-d6cc-4886-d4d0-277daa21c0c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'the', 'last', 'session', 'of', 'ai', '/', 'ml', 'course', '.']\n",
            "[101, 2023, 2003, 1996, 2197, 5219, 1997, 9932, 1013, 19875, 2607, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass, get hidden states\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "\n",
        "print(outputs)\n",
        "\n",
        "# Extract the hidden states from the output\n",
        "hidden_states = outputs[0]\n",
        "print(hidden_states)\n",
        "\n",
        "word_representation = hidden_states[0]\n",
        "print(word_representation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSXt1DnWEZuF",
        "outputId": "c7182332-3dec-4369-d0af-68550734f36e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3580, -0.0250,  0.2351,  ..., -0.3656,  0.1550,  0.3390],\n",
            "         [-0.6572, -0.6982, -0.2003,  ..., -0.1571,  0.5329, -0.2065],\n",
            "         [-0.6760, -0.6005,  0.4523,  ...,  0.0308,  0.1842,  0.4794],\n",
            "         ...,\n",
            "         [ 0.7289, -0.2130,  0.3197,  ...,  0.2856, -0.5340, -0.0213],\n",
            "         [ 0.8087,  0.1905, -0.3884,  ...,  0.0236, -0.6671, -0.3784],\n",
            "         [-0.5319,  0.1416,  0.2567,  ...,  0.0173, -0.3320, -0.0277]]]), pooler_output=tensor([[-0.9501, -0.5846, -0.9602,  0.8968,  0.6309, -0.2747,  0.9399,  0.5013,\n",
            "         -0.7919, -1.0000, -0.3180,  0.9130,  0.9840,  0.6651,  0.9250, -0.7721,\n",
            "         -0.3486, -0.6704,  0.4965, -0.7313,  0.7469,  1.0000,  0.1154,  0.5230,\n",
            "          0.6156,  0.9812, -0.7208,  0.9328,  0.9745,  0.7941, -0.8171,  0.3915,\n",
            "         -0.9906, -0.4003, -0.9721, -0.9973,  0.5069, -0.8072, -0.2521, -0.2444,\n",
            "         -0.9312,  0.4838,  1.0000, -0.0173,  0.4456, -0.4658, -1.0000,  0.4727,\n",
            "         -0.9221,  0.9171,  0.8705,  0.8378,  0.4423,  0.6643,  0.5833, -0.1693,\n",
            "          0.0964,  0.3640, -0.4062, -0.7659, -0.7041,  0.4750, -0.8374, -0.9510,\n",
            "          0.8299,  0.8461, -0.3299, -0.4490, -0.3372,  0.1658,  0.9518,  0.3903,\n",
            "         -0.3464, -0.8411,  0.6646,  0.5388, -0.7567,  1.0000, -0.5999, -0.9804,\n",
            "          0.9093,  0.8198,  0.7039, -0.4138,  0.6586, -1.0000,  0.6610, -0.3192,\n",
            "         -0.9906,  0.3815,  0.6175, -0.3970,  0.7481,  0.7480, -0.7134, -0.5341,\n",
            "         -0.5083, -0.8193, -0.5492, -0.2843,  0.2368, -0.4732, -0.5568, -0.4952,\n",
            "          0.5113, -0.6705, -0.6633,  0.7106,  0.1373,  0.7769,  0.5821, -0.5642,\n",
            "          0.5181, -0.9674,  0.7485, -0.5195, -0.9891, -0.7534, -0.9880,  0.8496,\n",
            "         -0.3675, -0.3553,  0.9629, -0.1611,  0.5583, -0.3500, -0.9214, -1.0000,\n",
            "         -0.7254, -0.6676, -0.1634, -0.3879, -0.9844, -0.9662,  0.7595,  0.9616,\n",
            "          0.4774,  1.0000, -0.5282,  0.9568, -0.3563, -0.7427,  0.4221, -0.5966,\n",
            "          0.8437,  0.4339, -0.8067,  0.3643, -0.3625,  0.4605, -0.7525, -0.4098,\n",
            "         -0.8284, -0.9405, -0.5190,  0.9595, -0.6343, -0.9475, -0.0264, -0.4071,\n",
            "         -0.5749,  0.9011,  0.7155,  0.5170, -0.4262,  0.6023,  0.5985,  0.7077,\n",
            "         -0.9026, -0.1021,  0.6501, -0.4702, -0.9267, -0.9832, -0.5400,  0.6827,\n",
            "          0.9882,  0.8663,  0.4945,  0.8332, -0.5516,  0.7271, -0.9787,  0.9805,\n",
            "         -0.3476,  0.4083, -0.3144,  0.6373, -0.8883,  0.2593,  0.9061, -0.6111,\n",
            "         -0.8548, -0.3023, -0.5783, -0.5752, -0.8095,  0.7212, -0.5232, -0.5492,\n",
            "         -0.2987,  0.9099,  0.9934,  0.7895,  0.3698,  0.6999, -0.9411, -0.5375,\n",
            "          0.3614,  0.4208,  0.3857,  0.9925, -0.6110, -0.3491, -0.9503, -0.9895,\n",
            "          0.1361, -0.9246, -0.3156, -0.7988,  0.7970, -0.1387,  0.5839,  0.5606,\n",
            "         -0.9956, -0.8175,  0.3883, -0.6180,  0.6430, -0.4559,  0.6844,  0.9673,\n",
            "         -0.7433,  0.7786,  0.9429, -0.9593, -0.8066,  0.8686, -0.4854,  0.9468,\n",
            "         -0.7702,  0.9978,  0.9455,  0.7035, -0.9522, -0.8543, -0.9487, -0.8235,\n",
            "         -0.2097,  0.1859,  0.8898,  0.8010,  0.5026,  0.0639, -0.6757,  0.9992,\n",
            "         -0.6582, -0.9639, -0.2744, -0.2740, -0.9905,  0.8927,  0.4444,  0.4800,\n",
            "         -0.6680, -0.7542, -0.9646,  0.9509,  0.2582,  0.9972, -0.2003, -0.9469,\n",
            "         -0.6162, -0.9292,  0.0213, -0.4948, -0.2724,  0.0448, -0.9619,  0.5784,\n",
            "          0.6935,  0.6304, -0.8648,  0.9995,  1.0000,  0.9823,  0.8971,  0.9415,\n",
            "         -0.9999, -0.3212,  1.0000, -0.9910, -1.0000, -0.9386, -0.7670,  0.4866,\n",
            "         -1.0000, -0.2698, -0.1909, -0.9431,  0.7686,  0.9800,  0.9967, -1.0000,\n",
            "          0.9241,  0.9542, -0.7844,  0.9159, -0.4931,  0.9750,  0.6104,  0.4313,\n",
            "         -0.3965,  0.5157, -0.9611, -0.9278, -0.6496, -0.6851,  0.9991,  0.3440,\n",
            "         -0.8803, -0.9367,  0.3523, -0.1198,  0.0825, -0.9721, -0.4438,  0.5550,\n",
            "          0.8006,  0.3322,  0.4810, -0.7940,  0.5306,  0.2459,  0.5336,  0.7756,\n",
            "         -0.9533, -0.8113, -0.2446,  0.0797, -0.7356, -0.9742,  0.9733, -0.6105,\n",
            "          0.9331,  1.0000,  0.3983, -0.9311,  0.6293,  0.4710, -0.6634,  1.0000,\n",
            "          0.8528, -0.9835, -0.7456,  0.7464, -0.6916, -0.7740,  0.9999, -0.4865,\n",
            "         -0.6930, -0.4813,  0.9819, -0.9911,  0.9973, -0.9380, -0.9664,  0.9779,\n",
            "          0.9397, -0.6228, -0.8798,  0.3768, -0.8444,  0.4509, -0.9683,  0.7452,\n",
            "          0.6533, -0.3458,  0.8907, -0.9208, -0.7402,  0.5445, -0.5856, -0.2265,\n",
            "          0.9611,  0.6276, -0.4271,  0.1694, -0.3746, -0.3962, -0.9761,  0.6324,\n",
            "          1.0000, -0.2850,  0.7983, -0.5152, -0.1679,  0.0597,  0.6663,  0.6992,\n",
            "         -0.5577, -0.9187,  0.8149, -0.9641, -0.9912,  0.8199,  0.4317, -0.4014,\n",
            "          1.0000,  0.5484,  0.3828,  0.3372,  0.9780,  0.1102,  0.6065,  0.8408,\n",
            "          0.9826, -0.4703,  0.7317,  0.9215, -0.8899, -0.4251, -0.7914,  0.1612,\n",
            "         -0.9516, -0.1568, -0.9715,  0.9703,  0.9583,  0.5144,  0.4391,  0.8211,\n",
            "          1.0000, -0.5880,  0.6211, -0.5124,  0.8831, -0.9999, -0.8664, -0.5047,\n",
            "         -0.2730, -0.7986, -0.5041,  0.4604, -0.9787,  0.8079,  0.6478, -0.9925,\n",
            "         -0.9912, -0.4103,  0.9263,  0.3836, -0.9887, -0.6154, -0.6578,  0.6232,\n",
            "         -0.5665, -0.9385, -0.1430, -0.4723,  0.6441, -0.4924,  0.7320,  0.8663,\n",
            "          0.5794, -0.8050, -0.2606, -0.2475, -0.9005,  0.8380, -0.8970, -0.9346,\n",
            "         -0.2945,  1.0000, -0.6442,  0.9448,  0.8223,  0.8130, -0.4522,  0.3708,\n",
            "          0.9616,  0.4127, -0.8224, -0.8700, -0.6788, -0.5669,  0.7649,  0.6513,\n",
            "          0.8645,  0.8449,  0.7386,  0.3112, -0.1946,  0.2736,  0.9999, -0.2469,\n",
            "         -0.3559, -0.7044, -0.2779, -0.5439, -0.4318,  1.0000,  0.4754,  0.6150,\n",
            "         -0.9903, -0.8694, -0.9593,  1.0000,  0.8963, -0.8890,  0.7590,  0.6802,\n",
            "         -0.2933,  0.8318, -0.4867, -0.3946,  0.3679,  0.2557,  0.9651, -0.6279,\n",
            "         -0.9691, -0.8054,  0.6074, -0.9626,  1.0000, -0.7659, -0.5342, -0.5271,\n",
            "         -0.2128,  0.7392,  0.1656, -0.9822, -0.4232,  0.2283,  0.9711,  0.4746,\n",
            "         -0.7475, -0.9301,  0.8498,  0.7902, -0.9117, -0.9424,  0.9717, -0.9816,\n",
            "          0.5523,  1.0000,  0.4857,  0.1700,  0.3949, -0.6986,  0.5265, -0.5230,\n",
            "          0.7110, -0.9643, -0.5065, -0.3910,  0.5090, -0.3359, -0.3936,  0.7705,\n",
            "          0.3498, -0.6812, -0.7277, -0.3679,  0.6262,  0.8960, -0.3541, -0.4330,\n",
            "          0.2252, -0.1903, -0.9431, -0.5804, -0.6385, -1.0000,  0.7835, -1.0000,\n",
            "          0.4843,  0.3490, -0.3680,  0.8543,  0.4399,  0.7646, -0.8014, -0.7986,\n",
            "          0.2811,  0.7627, -0.5021, -0.5275, -0.7821,  0.5939, -0.3649,  0.3821,\n",
            "         -0.6812,  0.7887, -0.4233,  1.0000,  0.2610, -0.7106, -0.9854,  0.4517,\n",
            "         -0.5151,  1.0000, -0.9550, -0.9632,  0.5206, -0.8072, -0.8890,  0.5515,\n",
            "          0.1620, -0.8213, -0.9530,  0.9472,  0.9128, -0.7056,  0.5324, -0.5144,\n",
            "         -0.7305,  0.2960,  0.9120,  0.9905,  0.5317,  0.9538,  0.2137, -0.3676,\n",
            "          0.9639,  0.4097,  0.7207,  0.3406,  1.0000,  0.5940, -0.9454, -0.0507,\n",
            "         -0.9852, -0.4359, -0.9632,  0.4406,  0.4022,  0.9366, -0.4748,  0.9729,\n",
            "         -0.8463,  0.2374, -0.4698, -0.6604,  0.4518, -0.9410, -0.9881, -0.9909,\n",
            "          0.7068, -0.6373, -0.2861,  0.4136,  0.3653,  0.6330,  0.5763, -1.0000,\n",
            "          0.9505,  0.6252,  0.9252,  0.9740,  0.7028,  0.5102,  0.4097, -0.9870,\n",
            "         -0.9887, -0.5207, -0.4989,  0.8884,  0.7768,  0.8962,  0.4790, -0.5799,\n",
            "         -0.5793, -0.7350, -0.4875, -0.9926,  0.6141, -0.6792, -0.9810,  0.9664,\n",
            "          0.1492, -0.2822, -0.0367, -0.8219,  0.9800,  0.9324,  0.6067,  0.2366,\n",
            "          0.6170,  0.9110,  0.9698,  0.9912, -0.9089,  0.9022, -0.5748,  0.5874,\n",
            "          0.6966, -0.9643,  0.3322,  0.4140, -0.4487,  0.4727, -0.3901, -0.9853,\n",
            "          0.6658, -0.4916,  0.6501, -0.6442, -0.1969, -0.6261, -0.3503, -0.8501,\n",
            "         -0.8191,  0.7687,  0.4432,  0.9052,  0.8456, -0.2906, -0.8164, -0.3336,\n",
            "         -0.8120, -0.9423,  0.9707, -0.2423, -0.4930,  0.7696,  0.2096,  0.8337,\n",
            "          0.4546, -0.5739, -0.4063, -0.8210,  0.9369, -0.5114, -0.6847, -0.6453,\n",
            "          0.8293,  0.4882,  1.0000, -0.8017, -0.8961, -0.4130, -0.4830,  0.5652,\n",
            "         -0.6013, -1.0000,  0.5340, -0.5127,  0.8269, -0.5763,  0.8463, -0.5612,\n",
            "         -0.9830, -0.4575,  0.5293,  0.6982, -0.6756, -0.7034,  0.7133, -0.2099,\n",
            "          0.9614,  0.9363, -0.0531,  0.0668,  0.7729, -0.8651, -0.7435,  0.9211]]), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
            "tensor([[[-0.3580, -0.0250,  0.2351,  ..., -0.3656,  0.1550,  0.3390],\n",
            "         [-0.6572, -0.6982, -0.2003,  ..., -0.1571,  0.5329, -0.2065],\n",
            "         [-0.6760, -0.6005,  0.4523,  ...,  0.0308,  0.1842,  0.4794],\n",
            "         ...,\n",
            "         [ 0.7289, -0.2130,  0.3197,  ...,  0.2856, -0.5340, -0.0213],\n",
            "         [ 0.8087,  0.1905, -0.3884,  ...,  0.0236, -0.6671, -0.3784],\n",
            "         [-0.5319,  0.1416,  0.2567,  ...,  0.0173, -0.3320, -0.0277]]])\n",
            "tensor([[-0.3580, -0.0250,  0.2351,  ..., -0.3656,  0.1550,  0.3390],\n",
            "        [-0.6572, -0.6982, -0.2003,  ..., -0.1571,  0.5329, -0.2065],\n",
            "        [-0.6760, -0.6005,  0.4523,  ...,  0.0308,  0.1842,  0.4794],\n",
            "        ...,\n",
            "        [ 0.7289, -0.2130,  0.3197,  ...,  0.2856, -0.5340, -0.0213],\n",
            "        [ 0.8087,  0.1905, -0.3884,  ...,  0.0236, -0.6671, -0.3784],\n",
            "        [-0.5319,  0.1416,  0.2567,  ...,  0.0173, -0.3320, -0.0277]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tensor to a numpy array\n",
        "word_representation_numpy = word_representation.numpy()\n",
        "\n",
        "# Extracting tokens from tokenizer\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
        "\n",
        "# Convert numpy array to a pandas DataFrame\n",
        "df = pd.DataFrame(word_representation_numpy, index=tokens)\n",
        "\n",
        "# Set column names as embedding_dim_0, embedding_dim_1, etc.\n",
        "df.columns = [f'embedding_dim_{i}' for i in range(word_representation_numpy.shape[1])]\n",
        "\n",
        "# Display the DataFrame\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "RiH8E3-MF1fx",
        "outputId": "edf2ce8d-31fb-4e47-a5f7-0c18309f0522"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         embedding_dim_0  embedding_dim_1  embedding_dim_2  embedding_dim_3  \\\n",
              "[CLS]          -0.357968        -0.025011         0.235098        -0.091520   \n",
              "this           -0.657183        -0.698246        -0.200257        -0.293208   \n",
              "is             -0.675955        -0.600480         0.452305        -0.117508   \n",
              "the            -0.744336        -0.607774         0.410556        -0.093455   \n",
              "last           -0.169711        -0.566675         1.139931         0.263556   \n",
              "session         0.158174        -0.031431         0.032985        -0.081283   \n",
              "of             -0.458881         0.309886         0.091665        -0.656385   \n",
              "ai             -0.453237         0.138157         0.362522        -0.473861   \n",
              "/              -0.236299        -0.330371         0.267579        -0.167829   \n",
              "ml             -0.044216        -0.840455         0.423911        -0.240436   \n",
              "course          0.728918        -0.213025         0.319690         0.060045   \n",
              ".               0.808732         0.190453        -0.388440         0.207054   \n",
              "[SEP]          -0.531898         0.141571         0.256702         0.355742   \n",
              "\n",
              "         embedding_dim_4  embedding_dim_5  embedding_dim_6  embedding_dim_7  \\\n",
              "[CLS]          -0.407983        -0.491192         0.748800         0.591719   \n",
              "this            0.373259        -0.378906         0.089233         1.332639   \n",
              "is              0.090807        -0.694865         0.470758         1.032447   \n",
              "the             0.387300        -0.671389         0.276390         1.018139   \n",
              "last           -0.337775        -0.194454         0.809776         0.690075   \n",
              "session        -0.187248        -0.753298         1.084636         0.389594   \n",
              "of              0.512449        -0.395260         1.132027         0.579112   \n",
              "ai              0.786913         0.193629         0.357376         0.115303   \n",
              "/               0.692405        -0.318068         0.500881         0.704060   \n",
              "ml             -0.101604         0.046052         0.675716        -0.156269   \n",
              "course          0.187559        -0.459077         0.562674         0.418571   \n",
              ".              -0.257577        -0.680683         0.693627        -0.452980   \n",
              "[SEP]           0.014337        -0.347830         0.941457         1.255964   \n",
              "\n",
              "         embedding_dim_8  embedding_dim_9  ...  embedding_dim_758  \\\n",
              "[CLS]          -0.042049         0.021393  ...           0.081465   \n",
              "this           -0.183366         0.320185  ...           0.867239   \n",
              "is             -0.277027         0.106946  ...           0.457346   \n",
              "the            -0.149591         0.286997  ...           0.548211   \n",
              "last            0.300043         0.443654  ...           0.269258   \n",
              "session        -0.335923         0.705883  ...          -0.059288   \n",
              "of             -0.329545         0.211426  ...           0.666989   \n",
              "ai             -0.132300        -0.510591  ...          -0.058973   \n",
              "/              -0.013894         0.594625  ...           0.216771   \n",
              "ml              0.139490        -0.075717  ...           0.376299   \n",
              "course          0.417116         0.333225  ...           0.429658   \n",
              ".               0.581172        -0.172066  ...           0.314237   \n",
              "[SEP]          -1.167319         0.338637  ...           0.599414   \n",
              "\n",
              "         embedding_dim_759  embedding_dim_760  embedding_dim_761  \\\n",
              "[CLS]             0.014005           0.517787           0.069586   \n",
              "this             -0.504051           1.009788          -0.221148   \n",
              "is               -0.206722           0.782795          -0.300906   \n",
              "the               0.082561           0.967757          -0.464343   \n",
              "last             -0.563727           0.457213          -0.641326   \n",
              "session           0.145946          -0.031880           0.024708   \n",
              "of                0.029743           0.531820          -0.078343   \n",
              "ai                0.295782           0.546885          -0.263775   \n",
              "/                 0.131904           0.274017          -0.100709   \n",
              "ml                0.393560           0.469674          -0.182568   \n",
              "course            0.450026           0.087760           0.074886   \n",
              ".                -0.240668           0.155684          -0.428801   \n",
              "[SEP]            -0.176941           0.972024           0.097324   \n",
              "\n",
              "         embedding_dim_762  embedding_dim_763  embedding_dim_764  \\\n",
              "[CLS]            -0.094016           0.294600           0.140071   \n",
              "this             -0.401614           0.639541           0.258121   \n",
              "is               -0.204903           0.216077           0.605646   \n",
              "the              -0.525179           0.321742           0.403398   \n",
              "last             -0.357631           0.054144           0.553309   \n",
              "session          -0.470789           0.596270           0.385298   \n",
              "of               -0.133534          -0.032729           0.123146   \n",
              "ai               -0.195554           1.238780           0.690225   \n",
              "/                 0.000178           0.901705           0.078792   \n",
              "ml               -0.396846           1.205963           0.522218   \n",
              "course           -0.351807           0.316662           0.256458   \n",
              ".                 0.103286          -0.371997           0.183868   \n",
              "[SEP]            -0.992683           0.476759           0.026013   \n",
              "\n",
              "         embedding_dim_765  embedding_dim_766  embedding_dim_767  \n",
              "[CLS]            -0.365574           0.154976           0.338950  \n",
              "this             -0.157102           0.532929          -0.206522  \n",
              "is                0.030832           0.184157           0.479357  \n",
              "the               0.163151           0.400861          -0.133964  \n",
              "last              0.103217          -0.066120          -0.278420  \n",
              "session           0.121989          -0.494081          -0.526524  \n",
              "of               -0.481668           0.276603          -0.089457  \n",
              "ai               -0.732256          -0.188194          -0.296405  \n",
              "/                -0.434113           0.109634          -0.075869  \n",
              "ml               -0.884017          -0.515486           0.430348  \n",
              "course            0.285588          -0.534007          -0.021289  \n",
              ".                 0.023606          -0.667086          -0.378433  \n",
              "[SEP]             0.017293          -0.332018          -0.027684  \n",
              "\n",
              "[13 rows x 768 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e002968b-a362-4d7d-a55f-c93ac190621d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>embedding_dim_0</th>\n",
              "      <th>embedding_dim_1</th>\n",
              "      <th>embedding_dim_2</th>\n",
              "      <th>embedding_dim_3</th>\n",
              "      <th>embedding_dim_4</th>\n",
              "      <th>embedding_dim_5</th>\n",
              "      <th>embedding_dim_6</th>\n",
              "      <th>embedding_dim_7</th>\n",
              "      <th>embedding_dim_8</th>\n",
              "      <th>embedding_dim_9</th>\n",
              "      <th>...</th>\n",
              "      <th>embedding_dim_758</th>\n",
              "      <th>embedding_dim_759</th>\n",
              "      <th>embedding_dim_760</th>\n",
              "      <th>embedding_dim_761</th>\n",
              "      <th>embedding_dim_762</th>\n",
              "      <th>embedding_dim_763</th>\n",
              "      <th>embedding_dim_764</th>\n",
              "      <th>embedding_dim_765</th>\n",
              "      <th>embedding_dim_766</th>\n",
              "      <th>embedding_dim_767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>[CLS]</th>\n",
              "      <td>-0.357968</td>\n",
              "      <td>-0.025011</td>\n",
              "      <td>0.235098</td>\n",
              "      <td>-0.091520</td>\n",
              "      <td>-0.407983</td>\n",
              "      <td>-0.491192</td>\n",
              "      <td>0.748800</td>\n",
              "      <td>0.591719</td>\n",
              "      <td>-0.042049</td>\n",
              "      <td>0.021393</td>\n",
              "      <td>...</td>\n",
              "      <td>0.081465</td>\n",
              "      <td>0.014005</td>\n",
              "      <td>0.517787</td>\n",
              "      <td>0.069586</td>\n",
              "      <td>-0.094016</td>\n",
              "      <td>0.294600</td>\n",
              "      <td>0.140071</td>\n",
              "      <td>-0.365574</td>\n",
              "      <td>0.154976</td>\n",
              "      <td>0.338950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>this</th>\n",
              "      <td>-0.657183</td>\n",
              "      <td>-0.698246</td>\n",
              "      <td>-0.200257</td>\n",
              "      <td>-0.293208</td>\n",
              "      <td>0.373259</td>\n",
              "      <td>-0.378906</td>\n",
              "      <td>0.089233</td>\n",
              "      <td>1.332639</td>\n",
              "      <td>-0.183366</td>\n",
              "      <td>0.320185</td>\n",
              "      <td>...</td>\n",
              "      <td>0.867239</td>\n",
              "      <td>-0.504051</td>\n",
              "      <td>1.009788</td>\n",
              "      <td>-0.221148</td>\n",
              "      <td>-0.401614</td>\n",
              "      <td>0.639541</td>\n",
              "      <td>0.258121</td>\n",
              "      <td>-0.157102</td>\n",
              "      <td>0.532929</td>\n",
              "      <td>-0.206522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is</th>\n",
              "      <td>-0.675955</td>\n",
              "      <td>-0.600480</td>\n",
              "      <td>0.452305</td>\n",
              "      <td>-0.117508</td>\n",
              "      <td>0.090807</td>\n",
              "      <td>-0.694865</td>\n",
              "      <td>0.470758</td>\n",
              "      <td>1.032447</td>\n",
              "      <td>-0.277027</td>\n",
              "      <td>0.106946</td>\n",
              "      <td>...</td>\n",
              "      <td>0.457346</td>\n",
              "      <td>-0.206722</td>\n",
              "      <td>0.782795</td>\n",
              "      <td>-0.300906</td>\n",
              "      <td>-0.204903</td>\n",
              "      <td>0.216077</td>\n",
              "      <td>0.605646</td>\n",
              "      <td>0.030832</td>\n",
              "      <td>0.184157</td>\n",
              "      <td>0.479357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.744336</td>\n",
              "      <td>-0.607774</td>\n",
              "      <td>0.410556</td>\n",
              "      <td>-0.093455</td>\n",
              "      <td>0.387300</td>\n",
              "      <td>-0.671389</td>\n",
              "      <td>0.276390</td>\n",
              "      <td>1.018139</td>\n",
              "      <td>-0.149591</td>\n",
              "      <td>0.286997</td>\n",
              "      <td>...</td>\n",
              "      <td>0.548211</td>\n",
              "      <td>0.082561</td>\n",
              "      <td>0.967757</td>\n",
              "      <td>-0.464343</td>\n",
              "      <td>-0.525179</td>\n",
              "      <td>0.321742</td>\n",
              "      <td>0.403398</td>\n",
              "      <td>0.163151</td>\n",
              "      <td>0.400861</td>\n",
              "      <td>-0.133964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>last</th>\n",
              "      <td>-0.169711</td>\n",
              "      <td>-0.566675</td>\n",
              "      <td>1.139931</td>\n",
              "      <td>0.263556</td>\n",
              "      <td>-0.337775</td>\n",
              "      <td>-0.194454</td>\n",
              "      <td>0.809776</td>\n",
              "      <td>0.690075</td>\n",
              "      <td>0.300043</td>\n",
              "      <td>0.443654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269258</td>\n",
              "      <td>-0.563727</td>\n",
              "      <td>0.457213</td>\n",
              "      <td>-0.641326</td>\n",
              "      <td>-0.357631</td>\n",
              "      <td>0.054144</td>\n",
              "      <td>0.553309</td>\n",
              "      <td>0.103217</td>\n",
              "      <td>-0.066120</td>\n",
              "      <td>-0.278420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>session</th>\n",
              "      <td>0.158174</td>\n",
              "      <td>-0.031431</td>\n",
              "      <td>0.032985</td>\n",
              "      <td>-0.081283</td>\n",
              "      <td>-0.187248</td>\n",
              "      <td>-0.753298</td>\n",
              "      <td>1.084636</td>\n",
              "      <td>0.389594</td>\n",
              "      <td>-0.335923</td>\n",
              "      <td>0.705883</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059288</td>\n",
              "      <td>0.145946</td>\n",
              "      <td>-0.031880</td>\n",
              "      <td>0.024708</td>\n",
              "      <td>-0.470789</td>\n",
              "      <td>0.596270</td>\n",
              "      <td>0.385298</td>\n",
              "      <td>0.121989</td>\n",
              "      <td>-0.494081</td>\n",
              "      <td>-0.526524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.458881</td>\n",
              "      <td>0.309886</td>\n",
              "      <td>0.091665</td>\n",
              "      <td>-0.656385</td>\n",
              "      <td>0.512449</td>\n",
              "      <td>-0.395260</td>\n",
              "      <td>1.132027</td>\n",
              "      <td>0.579112</td>\n",
              "      <td>-0.329545</td>\n",
              "      <td>0.211426</td>\n",
              "      <td>...</td>\n",
              "      <td>0.666989</td>\n",
              "      <td>0.029743</td>\n",
              "      <td>0.531820</td>\n",
              "      <td>-0.078343</td>\n",
              "      <td>-0.133534</td>\n",
              "      <td>-0.032729</td>\n",
              "      <td>0.123146</td>\n",
              "      <td>-0.481668</td>\n",
              "      <td>0.276603</td>\n",
              "      <td>-0.089457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ai</th>\n",
              "      <td>-0.453237</td>\n",
              "      <td>0.138157</td>\n",
              "      <td>0.362522</td>\n",
              "      <td>-0.473861</td>\n",
              "      <td>0.786913</td>\n",
              "      <td>0.193629</td>\n",
              "      <td>0.357376</td>\n",
              "      <td>0.115303</td>\n",
              "      <td>-0.132300</td>\n",
              "      <td>-0.510591</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.058973</td>\n",
              "      <td>0.295782</td>\n",
              "      <td>0.546885</td>\n",
              "      <td>-0.263775</td>\n",
              "      <td>-0.195554</td>\n",
              "      <td>1.238780</td>\n",
              "      <td>0.690225</td>\n",
              "      <td>-0.732256</td>\n",
              "      <td>-0.188194</td>\n",
              "      <td>-0.296405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>/</th>\n",
              "      <td>-0.236299</td>\n",
              "      <td>-0.330371</td>\n",
              "      <td>0.267579</td>\n",
              "      <td>-0.167829</td>\n",
              "      <td>0.692405</td>\n",
              "      <td>-0.318068</td>\n",
              "      <td>0.500881</td>\n",
              "      <td>0.704060</td>\n",
              "      <td>-0.013894</td>\n",
              "      <td>0.594625</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216771</td>\n",
              "      <td>0.131904</td>\n",
              "      <td>0.274017</td>\n",
              "      <td>-0.100709</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.901705</td>\n",
              "      <td>0.078792</td>\n",
              "      <td>-0.434113</td>\n",
              "      <td>0.109634</td>\n",
              "      <td>-0.075869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ml</th>\n",
              "      <td>-0.044216</td>\n",
              "      <td>-0.840455</td>\n",
              "      <td>0.423911</td>\n",
              "      <td>-0.240436</td>\n",
              "      <td>-0.101604</td>\n",
              "      <td>0.046052</td>\n",
              "      <td>0.675716</td>\n",
              "      <td>-0.156269</td>\n",
              "      <td>0.139490</td>\n",
              "      <td>-0.075717</td>\n",
              "      <td>...</td>\n",
              "      <td>0.376299</td>\n",
              "      <td>0.393560</td>\n",
              "      <td>0.469674</td>\n",
              "      <td>-0.182568</td>\n",
              "      <td>-0.396846</td>\n",
              "      <td>1.205963</td>\n",
              "      <td>0.522218</td>\n",
              "      <td>-0.884017</td>\n",
              "      <td>-0.515486</td>\n",
              "      <td>0.430348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>course</th>\n",
              "      <td>0.728918</td>\n",
              "      <td>-0.213025</td>\n",
              "      <td>0.319690</td>\n",
              "      <td>0.060045</td>\n",
              "      <td>0.187559</td>\n",
              "      <td>-0.459077</td>\n",
              "      <td>0.562674</td>\n",
              "      <td>0.418571</td>\n",
              "      <td>0.417116</td>\n",
              "      <td>0.333225</td>\n",
              "      <td>...</td>\n",
              "      <td>0.429658</td>\n",
              "      <td>0.450026</td>\n",
              "      <td>0.087760</td>\n",
              "      <td>0.074886</td>\n",
              "      <td>-0.351807</td>\n",
              "      <td>0.316662</td>\n",
              "      <td>0.256458</td>\n",
              "      <td>0.285588</td>\n",
              "      <td>-0.534007</td>\n",
              "      <td>-0.021289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.808732</td>\n",
              "      <td>0.190453</td>\n",
              "      <td>-0.388440</td>\n",
              "      <td>0.207054</td>\n",
              "      <td>-0.257577</td>\n",
              "      <td>-0.680683</td>\n",
              "      <td>0.693627</td>\n",
              "      <td>-0.452980</td>\n",
              "      <td>0.581172</td>\n",
              "      <td>-0.172066</td>\n",
              "      <td>...</td>\n",
              "      <td>0.314237</td>\n",
              "      <td>-0.240668</td>\n",
              "      <td>0.155684</td>\n",
              "      <td>-0.428801</td>\n",
              "      <td>0.103286</td>\n",
              "      <td>-0.371997</td>\n",
              "      <td>0.183868</td>\n",
              "      <td>0.023606</td>\n",
              "      <td>-0.667086</td>\n",
              "      <td>-0.378433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>[SEP]</th>\n",
              "      <td>-0.531898</td>\n",
              "      <td>0.141571</td>\n",
              "      <td>0.256702</td>\n",
              "      <td>0.355742</td>\n",
              "      <td>0.014337</td>\n",
              "      <td>-0.347830</td>\n",
              "      <td>0.941457</td>\n",
              "      <td>1.255964</td>\n",
              "      <td>-1.167319</td>\n",
              "      <td>0.338637</td>\n",
              "      <td>...</td>\n",
              "      <td>0.599414</td>\n",
              "      <td>-0.176941</td>\n",
              "      <td>0.972024</td>\n",
              "      <td>0.097324</td>\n",
              "      <td>-0.992683</td>\n",
              "      <td>0.476759</td>\n",
              "      <td>0.026013</td>\n",
              "      <td>0.017293</td>\n",
              "      <td>-0.332018</td>\n",
              "      <td>-0.027684</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13 rows × 768 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e002968b-a362-4d7d-a55f-c93ac190621d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e002968b-a362-4d7d-a55f-c93ac190621d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e002968b-a362-4d7d-a55f-c93ac190621d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e6d4658f-bbb2-4c3d-9adc-3106192b0b0b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6d4658f-bbb2-4c3d-9adc-3106192b0b0b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e6d4658f-bbb2-4c3d-9adc-3106192b0b0b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5b85bb4d-5444-4b97-89ff-59c398ec1323\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5b85bb4d-5444-4b97-89ff-59c398ec1323 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xne06ZL8u1mP"
      },
      "source": [
        "## <font color='green'>**Tutorial components** </font>\n",
        "\n",
        "**1. Predictions using existing fine-tuned model**\n",
        "\n",
        "**2. Fine-tuning of bert-base-uncased using AG_NEWS dataset**\n",
        "\n",
        "  2.1 Load the dataset from datasets\n",
        "\n",
        "  2.2 Tokenizing samples\n",
        "\n",
        "  2.3 Loading and Instantiating bert model (bert-base-uncased or bert-large-uncased)\n",
        "\n",
        "  2.4 Creating data loaders\n",
        "\n",
        "  2.5 Training\n",
        "\n",
        "  2.6 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HP_6Qx_YAsiQ"
      },
      "outputs": [],
      "source": [
        "!pip -q install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oWN1T5bUIL84"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMyPiaS7sa9R"
      },
      "source": [
        "## **1. Predictions using existing Fine-Tuned model on AG_NEWS dataset**\n",
        "\n",
        "https://huggingface.co/fabriceyhc/bert-base-uncased-ag_news\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ymKK2CjaznXk"
      },
      "outputs": [],
      "source": [
        "model_name = \"fabriceyhc/bert-base-uncased-ag_news\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjmtIAq6ZLTH",
        "outputId": "939e4f7c-6780-4f57-d185-7fbd5584fa25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Parameters:\n",
            "bert.embeddings.word_embeddings.weight: shape=torch.Size([30522, 768]), requires_grad=True\n",
            "bert.embeddings.position_embeddings.weight: shape=torch.Size([512, 768]), requires_grad=True\n",
            "bert.embeddings.token_type_embeddings.weight: shape=torch.Size([2, 768]), requires_grad=True\n",
            "bert.embeddings.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.embeddings.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.0.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.0.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.0.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.0.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.0.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.0.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.1.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.1.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.1.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.1.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.1.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.1.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.2.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.2.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.2.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.2.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.2.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.2.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.3.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.3.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.3.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.3.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.3.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.3.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.4.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.4.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.4.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.4.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.4.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.4.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.5.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.5.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.5.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.5.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.5.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.5.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.6.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.6.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.6.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.6.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.6.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.6.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.7.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.7.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.7.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.7.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.7.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.7.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.8.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.8.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.8.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.8.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.8.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.8.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.9.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.9.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.9.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.9.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.9.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.9.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.10.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.10.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.10.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.10.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.10.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.10.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.self.query.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.self.query.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.self.key.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.self.key.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.self.value.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.self.value.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.output.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.11.intermediate.dense.weight: shape=torch.Size([3072, 768]), requires_grad=True\n",
            "bert.encoder.layer.11.intermediate.dense.bias: shape=torch.Size([3072]), requires_grad=True\n",
            "bert.encoder.layer.11.output.dense.weight: shape=torch.Size([768, 3072]), requires_grad=True\n",
            "bert.encoder.layer.11.output.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.11.output.LayerNorm.weight: shape=torch.Size([768]), requires_grad=True\n",
            "bert.encoder.layer.11.output.LayerNorm.bias: shape=torch.Size([768]), requires_grad=True\n",
            "bert.pooler.dense.weight: shape=torch.Size([768, 768]), requires_grad=True\n",
            "bert.pooler.dense.bias: shape=torch.Size([768]), requires_grad=True\n",
            "classifier.weight: shape=torch.Size([4, 768]), requires_grad=True\n",
            "classifier.bias: shape=torch.Size([4]), requires_grad=True\n"
          ]
        }
      ],
      "source": [
        "# List all the model parameters\n",
        "model_params = list(model.named_parameters())\n",
        "print(\"Model Parameters:\")\n",
        "for param_name, param in model_params:\n",
        "    print(f\"{param_name}: shape={param.shape}, requires_grad={param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-1qqJndVzpUx"
      },
      "outputs": [],
      "source": [
        "# Example news articles\n",
        "news_articles = [\n",
        "    \"President Trump visits Japan for a summit meeting.\",\n",
        "    \"The new iPhone is expected to be released next month.\",\n",
        "    \"Scientists discover a new species of dinosaur in Argentina.\",\n",
        "    \"Stock market experiences a major crash, causing panic among investors.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP8113iwzrzM",
        "outputId": "f9c951a0-dfa9-4e9e-b4a2-746f412b9a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['president', 'trump', 'visits', 'japan', 'for', 'a', 'summit', 'meeting', '.']\n",
            "['the', 'new', 'iphone', 'is', 'expected', 'to', 'be', 'released', 'next', 'month', '.']\n",
            "['scientists', 'discover', 'a', 'new', 'species', 'of', 'dinosaur', 'in', 'argentina', '.']\n",
            "['stock', 'market', 'experiences', 'a', 'major', 'crash', ',', 'causing', 'panic', 'among', 'investors', '.']\n"
          ]
        }
      ],
      "source": [
        "# getting tokens of example news articles\n",
        "for i in range(len(news_articles)):\n",
        "  tokens = tokenizer.tokenize(news_articles[i])\n",
        "  print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTsqCxiIzupn",
        "outputId": "a751c803-4cf3-4389-8658-f627d4e1b016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Articles \n",
            " {'input_ids': tensor([[  101,  2343,  8398,  7879,  2900,  2005,  1037,  6465,  3116,  1012,\n",
            "           102,     0,     0,     0],\n",
            "        [  101,  1996,  2047, 18059,  2003,  3517,  2000,  2022,  2207,  2279,\n",
            "          3204,  1012,   102,     0],\n",
            "        [  101,  6529,  7523,  1037,  2047,  2427,  1997, 15799,  1999,  5619,\n",
            "          1012,   102,     0,     0],\n",
            "        [  101,  4518,  3006,  6322,  1037,  2350,  5823,  1010,  4786,  6634,\n",
            "          2426,  9387,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the news articles\n",
        "encoded_articles = tokenizer(news_articles, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "print(\"Encoded Articles \\n\", encoded_articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uumhq0kezxyr",
        "outputId": "b9d928fa-114e-48e1-c09f-3f52f258bab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outputs \n",
            " SequenceClassifierOutput(loss=None, logits=tensor([[ 5.7878, -2.2241, -1.8262, -2.2474],\n",
            "        [-0.7577, -4.3739, -0.8240,  4.6821],\n",
            "        [ 0.3683, -4.0031, -1.6276,  4.1006],\n",
            "        [ 0.8006, -4.8784,  3.0927, -1.0170]]), hidden_states=None, attentions=None)\n"
          ]
        }
      ],
      "source": [
        "# Get model predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**encoded_articles)\n",
        "\n",
        "print(\"Outputs \\n\", outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3QYdl_IWlHN9"
      },
      "outputs": [],
      "source": [
        "# Get the predicted class indices\n",
        "predicted_indices = torch.argmax(outputs.logits, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvZS7vB4lzL6",
        "outputId": "abac5129-8e3b-4b18-b138-ff18b777ba46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 3, 3, 2])\n"
          ]
        }
      ],
      "source": [
        "print(predicted_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbtV4RBunLKP"
      },
      "source": [
        "## **2. BERT-base-uncased Fine-Tuning using ag_news dataset**\n",
        "\n",
        "Dataset Link - https://huggingface.co/datasets/ag_news\n",
        "\n",
        "Bert-base-uncased paper Link - https://arxiv.org/pdf/1810.04805.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMfFWpXv05G0"
      },
      "source": [
        "### **2.1 Load the dataset from datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWeX1-OdIOT5",
        "outputId": "1b7cb846-3717-4928-f21b-033ef5b72fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 120000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 7600\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Load AG News dataset\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ae5ObBtYIhGF"
      },
      "outputs": [],
      "source": [
        "# Extract train, validation, and test data\n",
        "train_texts = dataset[\"train\"][\"text\"]\n",
        "train_labels = dataset[\"train\"][\"label\"]\n",
        "val_texts = dataset[\"test\"][\"text\"]\n",
        "val_labels = dataset[\"test\"][\"label\"]\n",
        "test_texts = dataset[\"test\"][\"text\"]\n",
        "test_labels = dataset[\"test\"][\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkU0z4Vq0_2n"
      },
      "source": [
        "### **2.2 Tokenizing samples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GrW8PoegIYDU"
      },
      "outputs": [],
      "source": [
        "# Define a simple dataset class for AG News\n",
        "class AGNewsDataset(Dataset):\n",
        "    def __init__(self, tokenizer, texts, labels):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoded_inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_inputs['input_ids'].squeeze()\n",
        "        attention_mask = encoded_inputs['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'label': label\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtZ1BaLP1EtM"
      },
      "source": [
        "\n",
        "### **2.3 Loading and Instantiating bert model (bert-base-uncased or bert-large-uncased)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSBfyRRg1-uu",
        "outputId": "95ad90fb-f67a-457e-d5a1-505f343e11ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Initialize tokenizer and model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWpU7dOK2EHe"
      },
      "source": [
        "### **2.4 Creating data loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MEj-jkT_IqQc"
      },
      "outputs": [],
      "source": [
        "# Create train dataset and dataloader\n",
        "train_dataset = AGNewsDataset(tokenizer, train_texts, train_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Create validation dataset and dataloader\n",
        "val_dataset = AGNewsDataset(tokenizer, val_texts, val_labels)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Create test dataset and dataloader\n",
        "test_dataset = AGNewsDataset(tokenizer, test_texts, test_labels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFTga4sB1ik1"
      },
      "source": [
        "### **2.5 Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cFYeHBlIIv3X"
      },
      "outputs": [],
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.1, eps=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S5W5hYDAp30",
        "outputId": "26b94c9f-4784-405f-af18-f9a7f8d800f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training: 100%|██████████| 1875/1875 [11:28<00:00,  2.72it/s, loss=0.297]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3 - Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9401    0.9072    0.9234     30000\n",
            "           1     0.9614    0.9822    0.9717     30000\n",
            "           2     0.8918    0.8722    0.8819     30000\n",
            "           3     0.8757    0.9067    0.8909     30000\n",
            "\n",
            "    accuracy                         0.9171    120000\n",
            "   macro avg     0.9173    0.9171    0.9170    120000\n",
            "weighted avg     0.9173    0.9171    0.9170    120000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Validation: 100%|██████████| 119/119 [00:20<00:00,  5.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3 - Validation Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9574    0.9474    0.9524      1900\n",
            "           1     0.9832    0.9868    0.9850      1900\n",
            "           2     0.8884    0.9221    0.9050      1900\n",
            "           3     0.9223    0.8937    0.9078      1900\n",
            "\n",
            "    accuracy                         0.9375      7600\n",
            "   macro avg     0.9379    0.9375    0.9375      7600\n",
            "weighted avg     0.9379    0.9375    0.9375      7600\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training: 100%|██████████| 1875/1875 [11:27<00:00,  2.73it/s, loss=0.0762]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/3 - Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9665    0.9493    0.9578     30000\n",
            "           1     0.9877    0.9920    0.9899     30000\n",
            "           2     0.9307    0.9152    0.9229     30000\n",
            "           3     0.9143    0.9419    0.9279     30000\n",
            "\n",
            "    accuracy                         0.9496    120000\n",
            "   macro avg     0.9498    0.9496    0.9496    120000\n",
            "weighted avg     0.9498    0.9496    0.9496    120000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Validation: 100%|██████████| 119/119 [00:19<00:00,  5.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/3 - Validation Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9674    0.9379    0.9524      1900\n",
            "           1     0.9900    0.9868    0.9884      1900\n",
            "           2     0.9416    0.8832    0.9115      1900\n",
            "           3     0.8766    0.9605    0.9166      1900\n",
            "\n",
            "    accuracy                         0.9421      7600\n",
            "   macro avg     0.9439    0.9421    0.9422      7600\n",
            "weighted avg     0.9439    0.9421    0.9422      7600\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training: 100%|██████████| 1875/1875 [11:28<00:00,  2.72it/s, loss=0.0534]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/3 - Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9783    0.9620    0.9701     30000\n",
            "           1     0.9923    0.9954    0.9938     30000\n",
            "           2     0.9475    0.9337    0.9406     30000\n",
            "           3     0.9316    0.9577    0.9445     30000\n",
            "\n",
            "    accuracy                         0.9622    120000\n",
            "   macro avg     0.9624    0.9622    0.9622    120000\n",
            "weighted avg     0.9624    0.9622    0.9622    120000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Validation: 100%|██████████| 119/119 [00:19<00:00,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/3 - Validation Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9523    0.9563    0.9543      1900\n",
            "           1     0.9899    0.9842    0.9871      1900\n",
            "           2     0.9295    0.8947    0.9118      1900\n",
            "           3     0.9037    0.9389    0.9210      1900\n",
            "\n",
            "    accuracy                         0.9436      7600\n",
            "   macro avg     0.9439    0.9436    0.9435      7600\n",
            "weighted avg     0.9439    0.9436    0.9435      7600\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_predicted_labels = []\n",
        "    train_true_labels = []\n",
        "\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\")\n",
        "    for batch in progress_bar:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_predicted_label = torch.argmax(outputs.logits, dim=1)\n",
        "        train_predicted_labels.extend(train_predicted_label.cpu().numpy())\n",
        "        train_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    train_classification_rep = classification_report(train_true_labels, train_predicted_labels, digits=4)\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs} - Training Classification Report:\\n{train_classification_rep}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_predicted_labels = []\n",
        "    val_true_labels = []\n",
        "\n",
        "    progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} - Validation\")\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            val_predicted_label = torch.argmax(outputs.logits, dim=1)\n",
        "            val_predicted_labels.extend(val_predicted_label.cpu().numpy())\n",
        "            val_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_classification_rep = classification_report(val_true_labels, val_predicted_labels, digits=4)\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs} - Validation Classification Report:\\n{val_classification_rep}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvUt78fU1oCJ"
      },
      "source": [
        "### **2.6 Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqhACJZBIWUv",
        "outputId": "64033642-78ff-40f7-c8cc-97b17e14373f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 7600/7600 [01:26<00:00, 87.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9523    0.9563    0.9543      1900\n",
            "           1     0.9899    0.9842    0.9871      1900\n",
            "           2     0.9295    0.8947    0.9118      1900\n",
            "           3     0.9037    0.9389    0.9210      1900\n",
            "\n",
            "    accuracy                         0.9436      7600\n",
            "   macro avg     0.9439    0.9436    0.9435      7600\n",
            "weighted avg     0.9439    0.9436    0.9435      7600\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Test set evaluation\n",
        "test_predicted_labels = []\n",
        "test_true_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader, desc=\"Test\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        test_predicted_label = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        test_predicted_labels.extend(test_predicted_label.cpu().numpy())\n",
        "        test_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_classification_rep = classification_report(test_true_labels, test_predicted_labels, digits=4)\n",
        "print(f\"\\nTest Set Classification Report:\\n{test_classification_rep}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}